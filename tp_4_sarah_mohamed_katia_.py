# -*- coding: utf-8 -*-
"""tp 4_Sarah_Mohamed_KATIA .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l-PUwZdHicIHXGpNMwT22merQE96iFMM
"""

import pandas as pd
data = pd.read_csv('diabetes.csv')
x = data.drop(['Outcome'],axis=1)
y = data ['Outcome']
print(x.shape, y.shape)

data.head()

from sklearn.ensemble import BaggingClassifier 
from sklearn.neighbors import KNeighborsClassifier 
bagging = BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, 
max_features=0.5)

from sklearn import tree
clf = tree.DecisionTreeClassifier() 
clf.fit(x, y) 
accuracy = clf.score(x,y) 
print(accuracy)

from sklearn.model_selection import train_test_split 
# 90% des données pour le test, 10% pour l'apprentissage 
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.90) 
 
clf = tree.DecisionTreeClassifier() 
clf.fit(x_train, y_train) 
 
Z = clf.predict(x_test) 
accuracy = clf.score(x_test,y_test) 
print(accuracy)

import seaborn 
seaborn.pairplot(data)

# Question 1 
accuracy = []
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.90)
for i in range (100):
  clf = tree.DecisionTreeClassifier() 
  clf.fit(x_train, y_train) 
  Z = clf.predict(x_test) 
  accuracy.append(clf.score(x_test,y_test))

import numpy as np 
print ('la moyenne', np.mean(accuracy))
print ('l ecart type', np.std(accuracy))
print('la variance ',np.var(accuracy))

clf2 = BaggingClassifier(tree.DecisionTreeClassifier(), 
max_samples=0.5, max_features=0.5, n_estimators=200)

clf2.fit(x_train, y_train) 
Z = clf2.predict(x_test) 
accuracy1=clf2.score(x_test,y_test)
print(accuracy1)

accuracy1 = []
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.90)
for i in range (100):
  clf2 = BaggingClassifier(tree.DecisionTreeClassifier(), max_samples=0.5,
                           max_features=0.5, n_estimators=200)
  clf2.fit(x_train, y_train) 
  Z = clf2.predict(x_test) 
  accuracy1.append(clf2.score(x_test,y_test))

print ('la moyenne', np.mean(accuracy1))
print ('l ecart type', np.std(accuracy1))
print('la variance ',np.var(accuracy1))

import matplotlib.pyplot as plt
estimator = 200 
accuracy1 = []
for i in range (1, estimator, 10):
  clf2 = BaggingClassifier(tree.DecisionTreeClassifier(), max_samples=0.5,
                           max_features=0.5, n_estimators= i)
  clf2.fit(x_train, y_train) 
  Z = clf2.predict(x_test) 
  accuracy1.append(clf2.score(x_test,y_test))
plt.plot([i for i in range (1, 200, 10)], accuracy1)
plt.plot()

from sklearn.model_selection import GridSearchCV
param = {'max_samples' : [0.2, 0.4, 0.6, 0.8 ], 'max_features': [0.1, 0.2, 0.3,
                                              0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]}
clf3= GridSearchCV(BaggingClassifier(tree.DecisionTreeClassifier()),param,
                   cv = 5)
clf3.fit(x_train,y_train)
print(clf3.best_params_,clf3.best_estimator_)

#Randomforest 
from sklearn.ensemble import RandomForestClassifier 
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.90, random_state=0)
clf4 = RandomForestClassifier(n_estimators=200) 
clf4.fit(x_train, y_train)

y_pred = clf4.predict(x_test) 
accuracy2 = clf4.score(x_test,y_test) 
print(accuracy2)

accuracy2 = []
for i in range (100):
  clf4 = RandomForestClassifier(n_estimators=200) 
  clf4.fit(x_train, y_train)
  Z = clf4.predict(x_test) 
  accuracy2.append(clf4.score(x_test,y_test))

print ('la moyenne', np.mean(accuracy2))
print ('l ecart type', np.std(accuracy2))
print('la variance ',np.var(accuracy2))

estimator = 200 
accuracy3 = []
for i in range (1, estimator, 10):
  clf4 = RandomForestClassifier(n_estimators=i) 
  clf4.fit(x_train, y_train)
  Z = clf4.predict(x_test) 
  accuracy3.append(clf4.score(x_test,y_test))
plt.plot([i for i in range (1, 200, 10)], accuracy3)
plt.plot()

from sklearn.ensemble import ExtraTreesClassifier
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.90, random_state=0)
clf4 = ExtraTreesClassifier(n_estimators=200) 
clf4.fit(x_train, y_train)

y_pred = clf4.predict(x_test) 
accuracy2 = clf4.score(x_test,y_test) 
print(accuracy2)

estimator = 200 
accuracy3 = []
for i in range (1, estimator, 10):
  clf4 = ExtraTreesClassifier(n_estimators=i) 
  clf4.fit(x_train, y_train)
  Z = clf4.predict(x_test) 
  accuracy3.append(clf4.score(x_test,y_test))
plt.plot([i for i in range (1, 200, 10)], accuracy3)
plt.plot()

#Boosting
from sklearn.ensemble import AdaBoostClassifier
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.90, 
                                                    random_state=0)
clf5= AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=5),
                         n_estimators=200, learning_rate=2) 
clf5.fit(x_train, y_train) 
accuracy = clf5.score(x_test, y_test) 
print(accuracy)

from sklearn.ensemble import AdaBoostClassifier

for i in range(1,10):
    clf27 = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=i),
    n_estimators=200, learning_rate=2)
    clf27.fit(x_train, y_train)
    accuracy = clf27.score(x_test, y_test)
    print("Le max_depth à", i, "a un accuracy de", accuracy)

from sklearn.ensemble import AdaBoostClassifier

for i in range(1,10):
    clf27 = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=5),
    n_estimators=200, learning_rate=i)
    clf27.fit(x_train, y_train)
    accuracy = clf27.score(x_test, y_test)
    print("Le learning_rate à", i, "a un accuracy de", accuracy)

